{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial is a brief introduction to music generation using **Generative Adversarial Networks** (**GAN**s). \n",
    "\n",
    "The goal of this tutorial is to train a machine learning model using a dataset of Bach compositions so that the model learns to add accompaniments to a single track input melody. In other words, if the user provides a single piano track of a song such as \"twinkle twinkle little star\", the GAN model would add three other piano tracks to make the music sound more Bach-inspired.\n",
    "\n",
    "The proposed algorithm consists of two competing networks: a generator and a critic (discriminator). A generator is a deep neural network that learns to create new synthetic data that resembles the distribution of the dataset on which it was trained. A critic is another deep neural network that is trained to differentiate between real and synthetic data. The generator and the critic are trained in alternating cycles such that the generator learns to produce more and more realistic data (Bach-like music in this use case) while the critic iteratively gets better at learning to differentiate real data (Bach music) from the synthetic ones.\n",
    "\n",
    "As a result, the quality of music produced by the generator gets more and more realistic with time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![High level WGAN-GP architecture](images/dgan.png \"WGAN-GP architecture\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies\n",
    "First, let's import all of the python packages we will use throughout the tutorial.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "import subprocess\n",
    "print(\"Please wait, while the required packages are being installed...\")\n",
    "subprocess.call(['./requirements.sh'], shell=True)\n",
    "print(\"All the required packages are installed successfully...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please wait, while the required packages are being installed...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "./requirements.sh: 2: source: not found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!\"\"\"pip install pypianoroll\n",
    "!pip install music21\n",
    "!pip install moviepy\n",
    "!pip install logging\"\"\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: -c: línea 0: EOF inesperado mientras se buscaba un «\"» coincidente\n",
      "/bin/bash: -c: línea 1: error sintáctico: no se esperaba el final del archivo\n",
      "Requirement already satisfied: music21 in /home/jesus/anaconda3/lib/python3.8/site-packages (6.7.1)\n",
      "Requirement already satisfied: more-itertools in /home/jesus/anaconda3/lib/python3.8/site-packages (from music21) (8.8.0)\n",
      "Requirement already satisfied: chardet in /home/jesus/anaconda3/lib/python3.8/site-packages (from music21) (4.0.0)\n",
      "Requirement already satisfied: webcolors in /home/jesus/anaconda3/lib/python3.8/site-packages (from music21) (1.11.1)\n",
      "Requirement already satisfied: joblib in /home/jesus/anaconda3/lib/python3.8/site-packages (from music21) (1.0.1)\n",
      "Requirement already satisfied: moviepy in /home/jesus/anaconda3/lib/python3.8/site-packages (1.0.3)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: numpy in /home/jesus/anaconda3/lib/python3.8/site-packages (from moviepy) (1.21.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/jesus/anaconda3/lib/python3.8/site-packages (from moviepy) (0.4.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/jesus/.local/lib/python3.8/site-packages (from moviepy) (2.9.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/jesus/anaconda3/lib/python3.8/site-packages (from moviepy) (4.62.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/jesus/anaconda3/lib/python3.8/site-packages (from moviepy) (2.26.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/jesus/anaconda3/lib/python3.8/site-packages (from moviepy) (0.1.9)\n",
      "Requirement already satisfied: pillow in /home/jesus/anaconda3/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (8.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jesus/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jesus/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jesus/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/jesus/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Installing collected packages: decorator\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.0.9\n",
      "    Uninstalling decorator-5.0.9:\n",
      "      Successfully uninstalled decorator-5.0.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 0.84.2 requires blinker, which is not installed.\n",
      "streamlit 0.84.2 requires click<8.0,>=7.0, but you have click 8.0.1 which is incompatible.\n",
      "streamlit 0.84.2 requires pandas<1.3.0,>=0.21.0, but you have pandas 1.3.0 which is incompatible.\n",
      "spyder 5.0.5 requires jedi==0.17.2, but you have jedi 0.18.0 which is incompatible.\n",
      "spyder 5.0.5 requires parso==0.7.0, but you have parso 0.8.2 which is incompatible.\u001b[0m\n",
      "Successfully installed decorator-4.4.2\n",
      "/bin/bash: -c: línea 0: EOF inesperado mientras se buscaba un «\"» coincidente\n",
      "/bin/bash: -c: línea 1: error sintáctico: no se esperaba el final del archivo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"pip install tensorflow-gpu==1.15.0\"\"\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pip install tensorflow-gpu==1.15.0'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# IMPORTS\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import logging\n",
    "import pypianoroll\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import music21\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.compat.v1.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Use this command to make a subset of GPUS visible to the jupyter notebook.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "# Utils library for plotting, loading and saving midi among other functions\n",
    "from utils import display_utils, metrics_utils, path_utils, inference_utils, midi_utils\n",
    "\n",
    "LOGGER = logging.getLogger(\"gan.train\")\n",
    "%matplotlib inline"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 19:14:22.328985: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-02 19:14:22.329043: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we configure paths to retrieve our dataset and save our experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_dir = './Experiments'\n",
    "\n",
    "# Directory to save checkpoints\n",
    "model_dir = os.path.join(root_dir,'2Bar')    # JSP: 229, Bach: 19199\n",
    "\n",
    "# Directory to save pianorolls during training\n",
    "train_dir = os.path.join(model_dir, 'train')\n",
    "\n",
    "# Directory to save checkpoint generated during training\n",
    "check_dir = os.path.join(model_dir, 'preload')\n",
    "\n",
    "# Directory to save midi during training\n",
    "sample_dir = os.path.join(model_dir, 'sample')\n",
    "\n",
    "# Directory to save samples generated during inference\n",
    "eval_dir = os.path.join(model_dir, 'eval')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "os.makedirs(sample_dir, exist_ok=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Dataset summary\n",
    "\n",
    "In this tutorial, we use the [`JSB-Chorales-dataset`](http://www-etud.iro.umontreal.ca/~boulanni/icml2012), comprising 229 chorale snippets. A chorale is a hymn that is usually sung with a single voice playing a simple melody and three lower voices providing harmony. In this dataset, these voices are represented by four piano tracks.\n",
    "\n",
    "Let's listen to a song from this dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_utils.playmidi('./original_midi/MIDI-0.mid')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv480'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv480');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQAFBABNVHJrAAAAFAD/UQMJJ8AA/1gEBAIYCIgA/y8ATVRyawAAAFEA/wMHdW5rbm93bgDAAADgAEAAwACIAJBgZIgAgGAAAJBhZIgAgGEAAJBjZJAAgGMAAJBhZIgAgGEAAJBgZIgAgGAAAJBeZJAAgF4AiAD/LwBNVHJrAAAAYwD/Awd1bmtub3duAMAAAOAAQADAAIgAkFxkkACAXAAAkFtkiACAWwAAkFxkhACAXAAAkFtkhACAWwAAkFlkhACAWQAAkFtkhACAWwAAkFxkiACAXAAAkFtkkACAWwCIAP8vAE1UcmsAAABRAP8DB3Vua25vd24AwAAA4ABAAMAAiACQV2SIAIBXAACQUGSEAIBQAACQUmSEAIBSAACQVGSQAIBUAACQVWSIAIBVAACQV2SYAIBXAIgA/y8ATVRyawAAAFoA/wMHdW5rbm93bgDAAADgAEAAwACIAJBQZIgAgFAAAJBNZIgAgE0AAJBIZIgAgEgAAJBNZIgAgE0AAJBGZIgAgEYAAJBEZIgAgEQAAJBLZJAAgEsAiAD/LwA=');\n",
       "                        });\n",
       "                </script>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data format - piano roll"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this tutorial, we represent music from the JSB-Chorales dataset in the piano roll format.\n",
    "\n",
    "**Piano roll** is a discrete representation of music which is intelligible by many machine learning algorithms. Piano rolls can be viewed as a two-dimensional grid with \"Time\" on the horizontal axis and \"Pitch\" on the vertical axis. A one or zero in any particular cell in this grid indicates if a note was played or not at that time for that pitch.\n",
    "\n",
    "Let us look at a few piano rolls in our dataset. In this example, a single piano roll track has 32 discrete time steps and 128 pitches. We see four piano rolls here, each one representing a separate piano track in the song."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/pianoroll2.png\" alt=\"Dataset summary\" width=\"800\">\n",
    "\n",
    "You might notice this representation looks similar to an image. While the sequence of notes is often the natural way that people view music, many modern machine learning models instead treat music as images and leverage existing techniques within the computer vision domain. You will see such techniques used in our architecture later in this tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why 32 time steps?**\n",
    "\n",
    "For the purpose of this tutorial, we sample two non-empty bars (https://en.wikipedia.org/wiki/Bar_(music)) from each song in the JSB-Chorales dataset. A **bar** (or **measure**) is a unit of composition and contains four beats for songs in our particular dataset (our songs are all in 4/4 time) :\n",
    "\n",
    "We’ve found that using a resolution of four time steps per beat captures enough of the musical detail in this dataset.\n",
    "\n",
    "This yields...\n",
    "\n",
    "$$ \\frac{4\\;timesteps}{1\\;beat} * \\frac{4\\;beats}{1\\;bar} * \\frac{2\\;bars}{1} = 32\\;timesteps $$\n",
    "\n",
    "Let us now load our dataset as a numpy array. Our dataset comprises 229 samples of 4 tracks (all tracks are piano). Each sample is a 32 time-step snippet of a song, so our dataset has a shape of...\n",
    "(num_samples, time_steps, pitch_range, tracks) = (229, 32, 128, 4)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_data = np.load('./dataset/train.npy')\n",
    "print(training_data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(229, 32, 128, 4)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see a sample of the data we'll feed into our model. The four graphs represent the four tracks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_utils.show_pianoroll(training_data)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABukAAAITCAYAAAAKFPH2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAABMbElEQVR4nO3dfZxkd10n+s+XmQkDTEOYFoZlWU2DCtwRXJIFAiwQso4PuPKQAJft6C73KqLXDSJ61RsBg+iu7O5dQLgrq+IicHsBmRGvGpUAQR7iw5rIUxZFTaNGYYAegxnCJJPhd/+omqTp6Z7p6u46Xafm/X696nW6zu9bv/pVna75JvOZc6paawEAAAAAAAC6c7ftXgAAAAAAAACcbYR0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0LFehnRV9e1V9a6quqmqvlRVN1bVr1bV41bUnVdV7TS3t27XawAAAAAAAODstXO7FzCqqnplkh9NspTknUk+n+Rrkzw9yaVV9a9ba29Z8bCPDGtX+vj4VgoAAAAAAACrq9badq9h3arqAUn+NsnnkjyytfbZZWNPSfLeJIuttQcP952XZDHJr7TWntf5ggEAAAAAAGAVfbvc5ddksOY/XB7QJUlr7ZoktyS533YsDAAAAAAAANarb5e7/PMktyd5TFV9VWvt8ycHqupJSWay+mUtH1hVL0gym8FlMn+/tfbRDtYLAAAAAAAAp+jV5S6TpKpelOQ/Z/BddO/MIHR7SJKnJXl/ku88eZbdsstdruZ9Sf5Na+2vx7pgAAAAAAAAWKF3IV2SVNUzkvxykvsu2/0XSX6ytbawrO7+Sf5tBmHejcPdj0xyZZKnDB/zT1trX1zHc163xtA3JDma5FMjvAQAundekn9orc1t90ImVVUtJrl39DSASXde9LQ16WcAvXFe9LPT0tMAeuO8bLCn9S6kq6ofTfLvkvxcktcl+UyShyX590m+Ocl/bK396Bnm2Jnkg0kem+RFrbXXrON51wrpvvEe97jHjoc//OHrfxEAdO4Tn/hEvvSlLx1prc1u91omVVUt3eMe99irpwFMNj3t9PQzgH7Qz85MTwPoh830tF59J11VXZTklUl+rbX24mVD11fVM5N8MskPV9XrW2s3rjJFkqS1dkdV/VIGId2TkpwxpGutXbDGmq57+MMffv51162V4QEwCS644IJcf/31n9rudUy4Tz384Q/fq6cBTDY97Yz0M4Ae0M/WRU8D6IHN9LS7bfFaxu1fDrfXrBxord2a5I8yeE2PWsdcnxtu77U1SwMAAAAAAID16VtId/fh9n5rjJ/cf/s65rpwuF3zjDsAAAAAAAAYh76FdB8Ybr+3qv7x8oGq+rYkT0hyLMm1w32PrapzVk5SVRcn+aHh3beMb7kAAAAAAABwql59J12SdyR5d5JvSvKJqvq1JJ9J8vAMLoVZSX68tbY0rH9lkv1V9b4kNw33PTLJxcOfX9pau7ajtQMAAAAAAECSnoV0rbUvV9VTk/xAkucmeWaSeyY5kuSqJD/XWnvXsoe8eVjz6CTflmRXksNJ3p7kda21DwQAAAAAAAA61quQLklaa8eTvHp4O1PtG5K8YcxLAgAAAAAAgJH07TvpAAAAAAAAoPeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHetlSFdV315V76qqm6rqS1V1Y1X9alU9bo36x1fVVVV1pKpuraqPVtWLqmpH12sHAAAAAACA3oV0VfXKJL+Z5Pwkv5PkNUmuT/L0JB+qqu9cUf/0JO9P8qQkv5bk/0lyTpJXJXlrdysHAAAAAACAgZ3bvYBRVNUDkvxIksNJHtla++yysackeW+Sn0ryluG+eyf5xSQnklzUWvvj4f6XDmufVVXPba0J6wAAAAAAAOhM386k+5oM1vyHywO6JGmtXZPkliT3W7b7WcP7bz0Z0A1rjyV5yfDu9491xQAAAAAAALBC30K6P09ye5LHVNVXLR+oqiclmUny7mW7Lx5uf2eVud6f5NYkj6+qu49hrQAAAAAAALCqXl3usrV2pKp+LMl/TvI/q+qdSZaSPCTJ05JcneQFyx7y0OH2k6vMdUdVLSbZn+TBST5xuueuquvWGHrYKK8BAAAAAAAAehXSJUlr7dVV9akkv5zk+cuG/iLJG1dcBvM+w+0X1pju5P5zt3KNAAAAAAAAcDp9u9xlqupHk7wjyRszOIPuXkkuSHJjkv+3qv7DKNMNt+1Mha21C1a7JfnTkV4AAAAAAAAAZ71ehXRVdVGSVyb5/1prL26t3dhau7W1dn2SZyb52yQ/XFUPHj7k5Jly9zllsoF7r6gDAAAAAACAsetVSJfkXw6316wcaK3dmuSPMnhNjxru/rPh9utX1lfVziRzSe7I4Cw8AAAAAAAA6ETfQrq7D7f3W2P85P7bh9v3Drffukrtk5LcM8m1rbXbtmZ5AAAAAAAAcGZ9C+k+MNx+b1X94+UDVfVtSZ6Q5FiSa4e735Hk80meW1X/bFnt7iQ/Pbz782NdMQAAAAAAAKywc7sXMKJ3JHl3km9K8omq+rUkn0ny8AwuhVlJfry1tpQkrbV/qKrnDx/3vqp6a5IjSZ6W5KHD/W/r/FUAAAAAAABwVutVSNda+3JVPTXJDyR5bpJnZnDJyiNJrkryc621d614zDur6slJfiLJpUl2J/mLJC8e1rcOXwIAAAAAAAD0K6RLktba8SSvHt7W+5gPJXnqmJYEAAAAAAAAI+nbd9IBAAAAAABA7wnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6tnO7FwDrsbSULCwki4vJ3FwyP5/Mzm6+dpxz93UdAAAAAADA+AnpmHhXX51cckly9Ohd+664Ijl0KDlwYOO145y7r+sAAAAAAAC64XKXTLSlpVNDpmRw/9JLB+MbqR3n3H1dBwAAAAAA0B0hHRNtYeHUkOmkW24ZjG+kdpxz93UdAAAAAABAd4R0TLTFxfWPj1I7zrn7ug4AAAAAAKA7Qjom2tzc+sdHqR3n3H1dBwAAAAAA0B0hHRNtfj7Zs2f1sZmZwfhGasc5d1/XAQAAAAAAdEdIx0SbnU0OHRqESsvNzCQHDw7GN1I7zrn7ug4AAAAAAKA7O7d7AXAmBw4Mvj9tYWGwnZtLLrss2bt3c7XjnLuv6wAAAAAAALohpKMXZmeTyy/f+tpxzt3XdQAAAAAAAOPncpcAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQsV6FdFX1vKpqZ7idWFZ/3hlq37qdrwcAAAAAAICz087tXsCIPpzk5WuMPTHJxUl+e5WxjyR55yr7P74lqwIAAAAAAIAR9Cqka619OIOg7hRV9fvDH39hleEPt9auHM+q+mVpKVlYSBYXk7m5ZH4+mZ3dmvpxzk13JuWY+/0AAAAAAGCa9SqkW0tVfUOSC5P8bZLf2ublTKyrr04uuSQ5evSufVdckRw6lBw4sLn6cc5NdyblmPv9AAAAAABg2vXqO+lO4wXD7RtaaydWGX9gVb2gqq4Ybh/Z5eImwdLSqaFHMrh/6aWD8Y3Wj3NuujMpx9zvBwAAAAAAZ4Peh3RVdY8k35nky0l+aY2yA0len+RnhtuPVNU1VfXVIzzPdavdkjxsky+hEwsLp4YeJ91yy2B8o/XjnJvuTMox9/sBAAAAAMDZoPchXZLnJDk3yW+31v5mxditSV6R5IIk9x3enpzkmiQXJXlPVd2rs5Vuo8XF0cZHqR/n3HRnUo653w8AAAAAAM4G0/CddN873P7XlQOttc8medmK3e+vqm9O8sEkj03yPUlec6Ynaa1dsNr+4dl054+y4O0wNzfa+Cj145yb7kzKMff7AQAAAADA2aDXZ9JV1f+S5PFJbkpy1Xof11q7I3ddGvNJY1jaxJmfT/bsWX1sZmYwvtH6cc5NdyblmPv9AAAAAADgbNDrkC7JC4bbN7TWToz42M8Nt2fF5S5nZ5NDhwYhx3IzM8nBg4PxjdaPc266MynH3O8HAAAAAABng95e7rKqdif5riRfTvKGDUxx4XB745YtasIdODD4Pq+FhcF2bi657LJk797N149zbrozKcfc7wcAAAAAANOutyFdkmcnuW+S32yt/c1qBVX12CR/0lq7fcX+i5P80PDuW8a6ygkzO5tcfvl46sc5N92ZlGPu9wMAAAAAgGnW55Due4fbXzhNzSuT7K+q92XwvXVJ8sgkFw9/fmlr7drxLA8AAAAAAABW18uQrqoenuSfZxC8XXWa0jcneWaSRyf5tiS7khxO8vYkr2utfWAr1nPTTclrX5vMz6/9fVlLS1956b7T1Y5aP+rcMG0m5fPlswgAAAAAwHr1MqRrrX0iSa2j7g3Z2PfVjeSzn01e+MLkiiuSQ4cG36e13NVXJ5dckhw9ete+tWpHrR91bpg2k/L58lkEAAAAAGAUd9vuBUyTo0eTSy8dnE1z0tLSqX9xv1btqPWjzg3TZlI+Xz6LAAAAAACMSki3xW65ZXC5u5MWFk79i/u1aketH3VumDaT8vnyWQQAAAAAYFRCujFYXFz95zPVjlo/6twwbSbl8+WzCAAAAADAqIR0YzA3t/rPZ6odtX7UuWHaTMrny2cRAAAAAIBRCem22MxMMj9/1/35+WTPnvXVjlo/6twwbSbl8+WzCAAAAADAqIR0W2hmJjl4MJmdvWvf7Gxy6NBg7Ey1o9aPOjdMm0n5fPksAgAAAAAwqp3bvYBpcP/7Jy95SXLZZcnevaeOHzgw+E6qhYXBdm5u7dpR60edG6bNpHy+fBYBAAAAABiFkG4LPOhByeWXn75mdvbMNRutH3VumDaT8vnyWQQAAAAAYL1c7hIAAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOjYzu1eAMDZamkpWVhIFheTublkfj6Znd187Tjn7us6AAAAAAAmjZAOYBtcfXVyySXJ0aN37bviiuTQoeTAgY3XjnPuvq4DAAAAAGASudwlQMeWlk4NmZLB/UsvHYxvpHacc/d1HQAAAAAAk0pIB9CxhYVTQ6aTbrllML6R2nHO3dd1AAAAAABMKiEdQMcWF9c/PkrtOOfu6zoAAAAAACaVkA6gY3Nz6x8fpXacc/d1HQAAAAAAk0pIB9Cx+flkz57Vx2ZmBuMbqR3n3H1dBwAAAADApBLSAXRsdjY5dGgQKi03M5McPDgY30jtOOfu6zoAAAAAACbVzu1eAMDZ6MCBwfenLSwMtnNzyWWXJXv3bq52nHP3dR0AAAAAAJNISAewTWZnk8sv3/racc7d13UAAAAAAEwal7sEAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjvUqpKuq51VVO8PtxCqPe3xVXVVVR6rq1qr6aFW9qKp2bMfrAAAAAAAA4Oy2c7sXMKIPJ3n5GmNPTHJxkt9evrOqnp7kYJJjSd6W5EiS70jyqiRPSPLsMa0VAAAAAAAAVtWrkK619uEMgrpTVNXvD3/8hWX77p3kF5OcSHJRa+2Ph/tfmuS9SZ5VVc9trb11jMsGAAAAAACAr9CrkG4tVfUNSS5M8rdJfmvZ0LOS3C/Jm04GdEnSWjtWVS9J8p4k359ESAcASW76h5vy2j98beYfMZ/Ze86uWrN061IWPraQxZsXM3fu3JbVjnPuSVlHX3k/uuP3GgAAAM4e1Vrb7jVsWlW9Nsm/TfJTrbWfXLb/LUkuSzLfWvvvKx6zM8kXkpyTZE9r7bYNPvd1559//vnXXXfdhtcPwPhdcMEFuf76669vrV2w3WuZVFV1Xf5Rzs8Lkj3n7Mmh5xzKgYcc+Iqaq//y6lzy9kty9Pajd+7bitpxzj0p6+gr70d3/F6zXnra6fl/NIB+0M/OTE8D6IfN9LS7jWNBXaqqeyT5ziRfTvJLK4YfOtx+cuXjWmt3JFnM4GzCB49zjQDQN0dvP5pL335plm5dunPf0q1Lp/zF/VbUjnPuSVlHX3k/uuP3GgAAAM4+vQ/pkjwnyblJfru19jcrxu4z3H5hjcee3H/umZ6kqq5b7ZbkYRtYMwBMvFtuvyULH1u48/7CxxZO+Yv7ragd59yTso6+8n50x+81AAAAnH2mIaT73uH2v27gsTXc9v+anwAwBos3L67681bWjnPuSVlHX3k/uuP3GgAAAM4+O7d7AZtRVf9LkscnuSnJVauUnDxT7j6rjCXJvVfUrWmta4kOz6Y7/0yPB4A+mjt3btWft7J2nHNPyjr6yvvRHb/XAAAAcPbp+5l0Lxhu39BaO7HK+J8Nt1+/cqCqdiaZS3JHkhvHszwA6K+Zc2Yy/4j5O+/PP2I+e87Zs+W145x7UtbRV96P7vi9BgAAgLNPb0O6qtqd5LuSfDnJG9Yoe+9w+62rjD0pyT2TXNtau23rVwgA/TVzzkwOPudgZu85e+e+2XvO5tBzDmXmnJktrR3n3JOyjr7yfnTH7zUAAACcfaq1fn4dW1V9V5I3JfnN1tp3rFFz7yR/mcFlLZ/QWvvj4f7dGQR4j0vyr1prb93EOq47//zzz7/uuus2OgUAHbjgggty/fXXX7/W5YsZ9LT7f+39z3/JW16Syx55WfbeY++qdUu3LmXhYwtZvHkxc+fObVntOOeelHX0lfejO36vWQ897fT8PxpAP+hnZ6anAfTDZnpan0O6DyT550me1lr7jdPUPSPJO5IcS/LWJEeSPC3JQ4f7n9M28SZolgD94H8Az0xPA+gHPe309DOAftDPzkxPA+iHzfS0Xl7usqoenkFAd1OSq05X21p7Z5InJ3l/kkuTXJ7keJIXJ3nuZgI6AAAAAAAA2Iid272AjWitfSJJjVD/oSRPHd+KAADWZ+VlBOcfMb/m93yNq3Yj9eMyztc4Ln19r8epj7/XfVwHAAAA06W3l7ucFE47B+gHl1I5Mz1t/K7+y6tzydsvydHbj965b885e3LoOYdy4CEHOqndSP24jPM1TsKaN1LfR338ve7jOpbT005PPwPoB/3szPQ0gH446y53CQDQN0u3Lp3yl/FJcvT2o7n07Zdm6dalsddupH5cxvkaJ2HNG6nvoz7+XvdxHQAAAEwnIR0AQAcWPrZwyl/Gn3TL7bdk4WMLY6/dSP24jPM1jktf3+tx6uPvdR/XAQAAwHQS0gEAdGDx5sV1j4+rdiP14zLO1zgufX2vx6mPv9d9XAcAAADTSUgHANCBuXPn1j0+rtqN1I/LOF/juPT1vR6nPv5e93EdAAAATCchHQBAB+YfMZ895+xZdWzmnJnMP2J+7LUbqR+Xcb7Gcenrez1Offy97uM6AAAAmE5COgCADszeczaHnnMoM+fMfMX+mXNmcvA5BzN7z9mx126kflzG+RonYc0bqe+jPv5e93EdAAAATKdqrW33Gnqtqq47//zzz7/uuuu2eykAnMYFF1yQ66+//vrW2gXbvZZJpad1Y+nWpSx8bCGLNy9m7ty5XPbIy7L3Hns7rd1I/biM8zWOS1/f63Hq4+91H9dxkp52evoZQD/oZ2empwH0w2Z6mpBukzRLgH7wP4BnpqcB9IOednr6GUA/6GdnpqcB9MNmeprLXQIAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdGzndi8AAADGYenWpSx8bCGLNy9m7ty5zD9iPrP3nN10LQAAAMBWENIBADB1rv7Lq3PJ2y/J0duP3rnvivdekUPPOZQDDzmw4VoAAACAreJylwAATJWlW5dOCd2S5OjtR3Pp2y/N0q1LG6oFAAAA2EpCOgAApsrCxxZOCd1OuuX2W7LwsYUN1QIAAABsJSEdAABTZfHmxXWPj1ILAAAAsJWEdAAATJW5c+fWPT5KLQAAAMBWEtIBADBV5h8xnz3n7Fl1bOacmcw/Yn5DtQAAAABbactDuqr6uqp6XVX9UVX9eVXduMrtL7f6eQEAIElm7zmbQ885lJlzZr5i/8w5Mzn4nIOZvefshmoBAAAAttLOrZysqh6X5N1J7pHkjiSHh9tTSrfyeQEAYLkDDzmQxR9czMLHFrJ482Lmzp3LZY+8LHvvsXdTtQAAAABbZUtDuiT/Psndk3xfkl9ura0W0AEAwNjN3nM2lz/28i2vBQAAANgKWx3SPTrJO1prv7DF8wIAAAAAAMDU2OrvpLs9yV9v8ZwAAAAAAAAwVbY6pLs2yaO2eE4AAAAAAACYKlsd0l2R5PFV9V1bPC8AAAAAAABMjU19J11VvWyV3e9N8saq+p4k1yW5eZWa1lp7xWaeGwAAAAAAAPpqUyFdkitPM/bE4W01LYmQDgAAAAAAgLPSZkO6p2zJKgAAAAAAAOAssqmQrrX2e1u1EAAAAAAAADhb3G27FwAAAAAAAABnmy0N6arqX1TVL1fVA9cYf+Bw/KKtfF4AAAAAAADok60+k+7yJI9vrf3daoPD/Y8b1m1KVT2xqg5W1aer6rbh9l1V9dRlNedVVTvN7a2bXQcAAAAAAACMalPfSbeK85O8+ww1H0zyzZt5kqp6SZJXJPl8kt9M8ukkX5XkUUkuSnLViod8JMk7V5nq45tZBwAAAAAAAGzEVod090+y6ll0yxwe1m1IVT07g4Du3Ukuaa3dsmJ81yoP+3Br7cqNPicAAAA9ctNNyWtfm8zPJ7Oza9ctLSULC8niYjI3d/r6cdXCtJqUz1cf5z4b1gEADLTWtuyWQQD3K2eo+ZUkn9/g/HdLcmOSLya53zrqz0vSkrxxK1/niue47vzzz28ATLbzzz+/JbmujakfTMNNTwPoBz1tHf0saS1pbc+e1t71rtXfyHe9azB+svZ09eOqhWk1KZ+vPs59NqxjSD9bZ0/z/2gAE28zPW2rv5Puj5I8o6oesNpgVT0wyTOGdRvx+CRzGVzO8u+r6tur6seq6ger6nGnedwDq+oFVXXFcPvIDT4/AAAAfXH0aHLppYOzO5ZbWkouuWQwfqb6cdXCtJqUz1cf5z4b1gEAfIWtDulem2QmyQeq6mlVdfckqaq7V9XTk7w/yZ4kP7fB+R893B5Ocn0G30f3s0leneTaqvq9qrrfKo87kOT1SX5muP1IVV1TVV+93ieuqutWuyV52AZfCwAAAON2yy2Dy68tt7Bw6l8mr1U/rlqYVpPy+erj3GfDOgCAr7ClIV1r7V0ZfF/cQ5L8WpIvVtXnMrg85aEkD07yitba72zwKU5+l933JblHkm/KIBT8hiS/m+RJSX51Wf2tw/VckOS+w9uTk1yT5KIk76mqe21wLQAAAPTB4uLp75+ufly1MK0m5fPVx7nPhnUAAF9h51ZP2Fr7yar6UJLLkzw2yblJjiT5gySvba1dvYnpdwy3leRZrbWPDO/fUFXPTPLJJE+uqse11n6/tfbZJC9bMcf7q+qbk3xwuL7vSfKadbyuC1bbPzyb7vzRXwoAAACdmJs7/f3T1Y+rFqbVpHy++jj32bAOAOArbPXlLpMMzqhrrX1Ha+3+rbVzhtunbTKgS5K/H25vXBbQnXzOL2VwNl2SPOYM67sjyS8N7z5pk2sCAABgUs3MJPPzX7lvfj7Zs2d99eOqhWk1KZ+vPs59NqwDAPgKWxrSVdW/rqpHnqHmEVX1rzf4FH823N68xvjJEO8e65jrc8Oty10CAABMo5mZ5ODBZHb2K/fPziaHDg3Gz1Q/rlqYVpPy+erj3GfDOgCAr7DVl7t8Y5Irk3z0NDVPS/JTSd60gfnfn+SOJF9XVee01m5fMf4Nw+2n1jHXhcPtjRtYBwAAAJPq/vdPXvKS5LLLkr17V685cGDwPUkLC4Pt3Nza9eOqhWk1KZ+vPs59NqwDALhTtda2brKqLye5srX2U6epeVmSl7XWNhQQVtVbklyW5Gdaay9Ztv9ABpe7/Ick57XWbq6qxyb5k5VhXlVdnOSqJHdP8oTW2rUbWctwruvOP//886+77rqNTgFABy644IJcf/3116/1HaPoaQB9oaednn4G0A/62ZnpaQD9sJmettVn0q3H1+euy1JuxIuTPDbJT1TVk5L8UZKvSfLMJCeSPL+1dvOw9pVJ9lfV+5LcNNz3yCQXD39+6WYCOgAAAAAAANiITYd0VfXLK3Y9o6rOW6V0R5KvTvLEJL+10edrrX12eIbcSzII5i5Mcstwzn/fWvuDZeVvHtY8Osm3JdmV5HCStyd5XWvtAxtdBwAAAAAAAGzUVpxJ97xlP7ck/3R4W01L8odJfmgzT9haO5LBGXUvPkPdG5K8YTPPBQAAAAAAAFttK0K6ueG2ktyY5NVJXrNK3Ykkf99a++IWPCcAAAAAAAD01qZDutbaX538uapenuSa5fsAAAAAAACAr7QVZ9LdqbX28q2cDwAAAAAAAKbRpkK6qvrq4Y9/21o7sez+GbXW/nozzw0AAAAAAAB9tdkz6T6VpCV5eJJPLrt/Jm0LnhsAAAAAAAB6abNB2ZsyCNy+sOI+AAAAAAAAsIZNhXStteed7j4AAAAAAABwqi275OTw++gencGZdP+jtfY3WzU3AAAAAAAATJMtCemq6j8leVGSGu5qVfWq1tr/uRXzAwAAAAAAwDTZdEhXVfNJXpzBGXR/mkFQ99AkL66q61tr/32zzwEAAABwVlpaShYWksXFZG4umZ9PZme3pn5ctQAArMtWnEn33UnuSPItrbVrkqSqvinJbw/HhHQAAAAAo7r66uSSS5KjR+/ad8UVyaFDyYEDm6sfVy0AAOt2ty2Y45FJ3nkyoEuS1tq7k/x6kn+6BfMDAAAAnF2Wlk4NxpLB/UsvHYxvtH5ctQAAjGQrQrr7JvmzVfb/aZJzt2B+AAAAgLPLwsKpwdhJt9wyGN9o/bhqAQAYyVaEdHdLcnyV/ccz+H46AAAAAEaxuDja+Cj146oFAGAkWxHSJUnbonkAAAAAmJsbbXyU+nHVAgAwkq0K6a6sqhPLb0leliQr9w9vd2zR8wIAAABMn/n5ZM+e1cdmZgbjG60fVy0AACPZqpCuRrxt1fMCAAAATJ/Z2eTQoUEQttzMTHLw4GB8o/XjqgUAYCQ7NztBa03gBgAAALDVDhwYfOfbwsJgOzeXXHZZsnfv5uvHVQsAwLptOqQDAAAAYExmZ5PLLx9P/bhqAQBYF2fBAQAAAAAAQMecSQcAAACMZmnpKy99OD9/+u8mG6V+XLV9nhsAgKkkpAMAAADW7+qrk0suSY4evWvfFVckhw4NvrtsM/Xjqu3z3AAATC2XuwQAAADWZ2np1IApGdy/9NLB+Ebrx1Xb57kBAJhqQjoAAABgfRYWTg2YTrrllsH4RuvHVdvnuQEAmGpCOgAAAGB9FhdHGx+lfly1fZ4bAICpJqQDAAAA1mdubrTxUerHVdvnuQEAmGpCOgAAAGB95ueTPXtWH5uZGYxvtH5ctX2eGwCAqSakAwAAANZndjY5dGgQKC03M5McPDgY32j9uGr7PDcAAFNt53YvAAAAAOiRAwcG3522sDDYzs0ll12W7N27+fpx1fZ5bgAAppaQDgAAABjN7Gxy+eXjqR9XbZ/nBgBgKrncJQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHSstyFdVT2xqg5W1aer6rbh9l1V9dRVah9fVVdV1ZGqurWqPlpVL6qqHduxdgAAAAAAAM5uO7d7ARtRVS9J8ookn0/ym0k+neSrkjwqyUVJrlpW+/QkB5McS/K2JEeSfEeSVyV5QpJnd7h0AAAAAAAA6F9IV1XPziCge3eSS1prt6wY37Xs53sn+cUkJ5Jc1Fr74+H+lyZ5b5JnVdVzW2tv7Wr9AAAAAAAA0KuQrqruluSVSW5NMr8yoEuS1trxZXefleR+Sd50MqAb1hwbno33niTfn0RIBwAAwPRZWkoWFpLFxWRuLpmfT2ZnN18LAABsWq9CuiSPTzKX5B1J/r6qvj3JN2RwKcs/aq39/or6i4fb31llrvdnEPY9vqru3lq7bUxrBgAAgO5dfXVyySXJ0aN37bviiuTQoeTAgY3XAgAAW6JvId2jh9vDSa5P8ojlg1X1/iTPaq19brjrocPtJ1dO1Fq7o6oWk+xP8uAknxjLigEAAKBrS0unhm7J4P6llw7Oljt5ltwotQAAwJa523YvYET3H26/L8k9knxTkpkMzqb73SRPSvKry+rvM9x+YY35Tu4/90xPXFXXrXZL8rDRXgIAAACM2cLCqaHbSbfcMhjfSC0AALBl+hbS7RhuK4Mz5t7TWjvaWrshyTOT3JTkyVX1uHXOV8Nt2+J1AgAAwPZZXFz/+Ci1AADAlunb5S7/fri9sbX2keUDrbUvVdXvJvnuJI9J8vu560y5+2R19x5u1zrTbvn8F6y2f3g23flnejwAAAB0Zm5u/eOj1AIAAFumb2fS/dlwe/Ma4ydDvHusqP/6lYVVtTPJXJI7kty4ResDAACA7Tc/n+zZs/rYzMxgfCO1AADAlulbSPf+DEK1r6uqc1YZ/4bh9lPD7XuH229dpfZJSe6Z5NrW2m1buUgAAADYVrOzyaFDg5BtuZmZ5ODBwfhGagEAgC3Tq8tdttY+X1VvS3JZkpclecnJsao6kORbMrh05e8Md78jySuTPLeqXtta++Nh7e4kPz2s+fmOlg8AAADdOXBg8H1yCwuD7dxcctllyd69m6sFAAC2RK9CuqEXJ3lskp+oqicl+aMkX5PkmUlOJHl+a+3mJGmt/UNVPT+DsO59VfXWJEeSPC3JQ4f739b5KwAAAIAuzM4ml1++9bUAAMCm9e1yl2mtfTaDkO5VSf5JkhcmuTjJbyV5YmvtV1fUvzPJkzO4VOalSS5PcjyDsO+5rbXW2eIBAAAAAAAg/TyTLq21IxmEbC9eZ/2Hkjx1rIsCAAAAAACAderdmXQAAAAAAADQd0I6AAAAAAAA6JiQDgAAAAAAADompAMAAAAAAICOCekAAAAAAACgY0I6AAAAAAAA6JiQDgAAAAAAADompAMAAAAAAICOCekAAAAAAACgYzu3ewEAAABwVltaShYWksXFZG4umZ9PZmc3XwtA791022157U03ZX7fvszu2rVqzdLx41k4fDiLx45lbvfu09aOWj/q3ACMRkgHAAAA2+Xqq5NLLkmOHr1r3xVXJIcOJQcObLwWgKnw2dtvzwv/4i9yxeJiDu3fnwN7937F+NVHjuSSG27I0RMn7ty3Vu2o9aPODcDoXO4SAAAAtsPS0qmhWzK4f+mlg/GN1AIwdY6eOJFLb7ghS8eP37lv6fjxU0K0tWpHrR91bgA2RkgHAAAA22Fh4dTQ7aRbbhmMb6QWgKl0y4kTWTh8+M77C4cPnxKirVU7av2ocwOwMUI6AAAA2A6Li+sfH6UWgKm1eOzYqj+fqXbU+lHnBmBjhHQAAACwHebm1j8+Si0AU2tu9+5Vfz5T7aj1o84NwMYI6QAAAGA7zM8ne/asPjYzMxjfSC0AU2lmx47M79t35/35ffuyZ8eOddWOWj/q3ABsjJAOAAAAtsPsbHLo0CBkW25mJjl4cDC+kVoAps7Mjh05uH9/ZnftunPf7K5dObR/f2ZWhGmr1Y5aP+rcAGzMzu1eAAAAAJy1DhwYfJ/cwsJgOzeXXHZZsnfv5moBmAr3P+ecvORrvzaX7duXvasEYwf27s3ihRdm4fDhLB47lrndu9esHbV+1LkBGJ2QDgAAALbT7Gxy+eVbXwtA7z3o7nfP5Q960GlrZnftOmPNRutHnRuA0bjcJQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHRMSAcAAAAAAAAdE9IBAAAAAABAx4R0AAAAAAAA0DEhHQAAAAAAAHSsdyFdVX2qqtoat8+sqD3vNLWtqt66Xa8DAAAAAACAs9fO7V7ABn0hyatX2X90jfqPJHnnKvs/vkXrAQAAAAAAgHXra0h3c2vtyhHqPzxiPQAAAAAA67R0/HgWDh/O4rFjmdu9O/P79mV2164tqR/n3H3kvYbp0deQDgAAAACACXD1kSO55IYbcvTEiTv3XbG4mEP79+fA3r2bqh/n3H3kvYbp0rvvpBu6e1V9Z1VdUVU/WFVPqaodp6l/YFW9YFj/gqp6ZGcrBQAAAACYUkvHj58S1CTJ0RMncukNN2Tp+PEN149z7j7yXsP06WtI94Akb07yMxl8N917k/x5VT15jfoDSV4/rH99ko9U1TVV9dXrfcKqum61W5KHbeaFAAAAAAD01cLhw6cENSfdcuJEFg4f3nD9OOfuI+81TJ8+hnT/Lcm/yCCou1eSRyT5r0nOS/LbVfWNy2pvTfKKJBckue/w9uQk1yS5KMl7qupeXS0cAAAAAGCaLB47NtL4KPXjnLuPvNcwfXr3nXSttZev2PXxJN9XVUeT/HCSK5M8c1j72SQvW1H//qr65iQfTPLYJN+T5DXreN4LVts/PJvu/BFeAgAAAADAVJjbvXuk8VHqxzl3H3mvYfr08Uy6tbx+uH3SmQpba3ck+aX11gMAAAAAcKr5ffuyZ8eOVcdmduzI/L59G64f59x95L2G6TNNId1nh9v1Xr7ycyPWAwAAAACwzOyuXTm0f39mVgQ2Mzt25OD+/ZndtWvD9eOcu4+81zB9ene5y9N43HB74zrrLxyxHgAAAACAFQ7s3ZvFCy/MwuHDWTx2LHO7d+eyffuyd42gZpT6cc7dR95rmC69Cumqan+ST7fWjqzY/zVJXje8+5Zl+x+b5E9aa7evqL84yQ+trAcAAAAAYHSzu3bl8gc9aCz145y7j7zXMD16FdIleXaSH6+qa5IsJrklyUOSfHuS3UmuSvKfltW/Msn+qnpfkpuG+x6Z5OLhzy9trV3bwboBAAAAAADgTn0L6a5J8tAkj8rg8pb3SnJzkg8meXOSN7fW2rL6Nyd5ZpJHJ/m2JLuSHE7y9iSva619oLOVAwAAAAAAwFCvQrrW2u8l+b0R6t+Q5A3jWxEAAAAAAACM7m7bvQAAAAAAAAA42wjpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAjgnpAAAAAAAAoGNCOgAAAAAAAOiYkA4AAAAAAAA6JqQDAAAAAACAju3c7gUAAAAAAACwNZaOH8/C4cNZPHYsc7t3Z37fvszu2rXp2nHOPSnr2Ej9ZgjpAAAAAAAApsDVR47kkhtuyNETJ+7cd8XiYg7t358De/duuHacc0/KOjZSv1kudwkAAAAAANBzS8ePnxIwJcnREydy6Q03ZOn48Q3VjnPuSVnHRuq3gpAOAAAAAACg5xYOHz4lYDrplhMnsnD48IZqxzn3pKxjI/VbQUgHAAAAAADQc4vHjq17fJTacc49KevYSP1WENIBAAAAAAD03Nzu3eseH6V2nHNPyjo2Ur8VhHQAAAAAAAA9N79vX/bs2LHq2MyOHZnft29DteOce1LWsZH6rSCkAwAAAAAA6LnZXbtyaP/+zKwImmZ27MjB/fszu2vXhmrHOfekrGMj9Vth55bPCAAAAAAAQOcO7N2bxQsvzMLhw1k8dixzu3fnsn37sneVgGmU2nHOPSnr2Ej9ZgnpAAAAAAAApsTsrl25/EEP2vLacc49KevYSP1muNwlAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQsd6FdFX1qapqa9w+s8ZjHl9VV1XVkaq6tao+WlUvqqodXa8fAAAAAAAAdm73AjboC0levcr+oyt3VNXTkxxMcizJ25IcSfIdSV6V5AlJnj22VQIAAAAAAMAq+hrS3dxau/JMRVV17yS/mOREkotaa3883P/SJO9N8qyqem5r7a3jXCwAAAAAAJNn6fjxLBw+nMVjxzK3e3fm9+3L7K5dm64d99zAdOhrSLdez0pyvyRvOhnQJUlr7VhVvSTJe5J8fxIhHQAAAADAWeTqI0dyyQ035OiJE3fuu2JxMYf278+BvXs3XDvuuYHp0bvvpBu6e1V9Z1VdUVU/WFVPWeP75S4ebn9nlbH3J7k1yeOr6u5jWykAAAAAABNl6fjxU4KxJDl64kQuveGGLB0/vqHacc8NTJe+hnQPSPLmJD+TwXfTvTfJn1fVk1fUPXS4/eTKCVprdyRZzOBswgef6Qmr6rrVbkketvGXAQAAAABA1xYOHz4lGDvplhMnsnD48IZqxz03MF36GNL9tyT/IoOg7l5JHpHkvyY5L8lvV9U3Lqu9z3D7hTXmOrn/3C1fJQAAAAAAE2nx2LF1j49SO+65genSu++ka629fMWujyf5vqo6muSHk1yZ5JnrnK5OTruO571g1QkGZ9Odv87nAwAAAABgm83t3r3u8VFqxz03MF36eCbdWl4/3D5p2b6TZ8rdJ6u794o6AAAAAACm3Py+fdmzY8eqYzM7dmR+374N1Y57bmC6TFNI99nh9l7L9v3ZcPv1K4urameSuSR3JLlxvEsDAAAAAGBSzO7alUP792dmRUA2s2NHDu7fn9lduzZUO+65genSu8tdnsbjhtvlgdt7k1yW5FuT/PcV9U9Kcs8k72+t3Tb+5QEAAAAAMCkO7N2bxQsvzMLhw1k8dixzu3fnsn37sneVYGyU2nHPDUyPXoV0VbU/yadba0dW7P+aJK8b3n3LsqF3JHllkudW1Wtba388rN+d5KeHNT8/3lUDAAAAADCJZnftyuUPetCW1457bmA69CqkS/LsJD9eVdckWUxyS5KHJPn2JLuTXJXkP50sbq39Q1U9P4Ow7n1V9dYkR5I8LclDh/vf1ukrAAAAAAAA4KzXt5DumgzCtUdlcHnLeyW5OckHk7w5yZtba235A1pr76yqJyf5iSSXZhDm/UWSFyf5uZX1AAAAAAAAMG69Culaa7+X5Pc28LgPJXnq1q8IAAAAAAAARne37V4AAAAAAAAAnG2EdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMd2bvcCAAAAAACA6bZ0/HgWDh/O4rFjmdu9O/P79mV2165N1457bhgnIR0AAAAAADA2Vx85kktuuCFHT5y4c98Vi4s5tH9/Duzdu+Hacc8N4+ZylwAAAAAAwFgsHT9+SjCWJEdPnMilN9yQpePHN1Q77rmhC0I6AAAAAABgLBYOHz4lGDvplhMnsnD48IZqxz03dEFIBwAAAAAAjMXisWPrHh+ldtxzQxeEdAAAAAAAwFjM7d697vFRasc9N3RBSAcAAAAAAIzF/L592bNjx6pjMzt2ZH7fvg3Vjntu6IKQDgAAAAAAGIvZXbtyaP/+zKwIyGZ27MjB/fszu2vXhmrHPTd0Yed2LwAAAAAAAJheB/buzeKFF2bh8OEsHjuWud27c9m+fdm7SjA2Su2454ZxE9IBAAAAAABjNbtrVy5/0IO2vHbcc8M4udwlAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQsZ3bvQAAAAAAAIBJs3T8eBYOH87isWOZ27078/v2ZXbXri2pH+fc9IeQDgAAAAAAYJmrjxzJJTfckKMnTty574rFxRzavz8H9u7dVP0456ZfXO4SAAAAAABgaOn48VNCsSQ5euJELr3hhiwdP77h+nHOTf8I6QAAAAAAAIYWDh8+JRQ76ZYTJ7Jw+PCG68c5N/0jpAMAAAAAABhaPHZspPFR6sc5N/0jpAMAAAAAABia2717pPFR6sc5N/0jpAMAAAAAABia37cve3bsWHVsZseOzO/bt+H6cc5N/wjpAAAAAAAAhmZ37cqh/fszsyIcm9mxIwf378/srl0brh/n3PTPzu1ewGZV1XcledPw7vNba7+0bOy8JIunefjbWmvPHePyAAAAAACAnjmwd28WL7wwC4cPZ/HYsczt3p3L9u3L3jVCsVHqxzk3/dLrkK6q/kmS1yY5mmTPaUo/kuSdq+z/+BiWBQAAAAAA9Nzsrl25/EEPGkv9OOemP3ob0lVVJflvSZaSHEryI6cp/3Br7cou1gUAAAAAAABn0ufvpHthkouT/G9JvrjNawEAAAAAAIB16+WZdFX18CQ/m+Q1rbX3V9XFZ3jIA6vqBUlmMzjz7vdbax8d9zoBAAAAAABgNb0L6apqZ5I3J/nrJFes82EHhrfl87wvyb9prf31li4QAAAAAAAAzqB3IV2SlyV5VJJ/3lr70hlqb03yiiTvTHLjcN8jk1yZ5ClJ3lNV/7S1dsbLZVbVdWsMfeMnPvGJXHDBBetYOgDb5ROf+ESSnLfNy5h05+lpAJNPTzsj/QygB/SzddHTAHpgMz2tWmtbuphxqqrHJLk2yX9urf3osv1XJvnJJM9vrf3SOubZmeSDSR6b5EWttdes4zFrhXSPSvLlJB854wtgkj1suP3TbV0Fm+EYTodxHsfzkvxDa21uDHNPhaq6LcmO6Gl95s/C6eA4Tgc9bZvoZ1PDn4XTwXHsP/1sG+lpU8Ofhf3nGE6HiexpvTmTbtllLj+Z5KWbmau1dkdV/VIGId2TkpwxpGutrfpPVk6Gd2uN0w+OY/85htPBcdx2H0+8/33mMzQdHMfp4DhuK/1sCvgMTQfHsf8cw22np00Bn6P+cwynw6Qex7tt9wJGsCfJ1yd5eJJjVdVO3jI4iy5JfnG479XrmO9zw+29tn6pAAAAAAAAsLbenEmX5LYkb1hj7PwMLjv5wSR/luT31zHfhcPtjaetAgAAAAAAgC3Wm5CutfalJN+z2tjwO+keleRXln8nXVU9NsmftNZuX1F/cZIfGt59y1gWDAAAAAAAAGvoTUi3Qa9Msr+q3pfkpuG+Rya5ePjzS1tr127HwgAAAAAAADh7TXtI9+Ykz0zy6CTflmRXksNJ3p7kda21D2zj2gAAAAAAADhLVWttu9cAAAAAAAAAZ5W7bfcCAAAAAAAA4GwjpAMAAAAAAICOCekAAAAAAACgY0I6AAAAAAAA6JiQDgAAAAAAADompAMAAAAAAICOCekAAAAAAACgY0K6DaqqB1XVL1fV31XVbVX1qap6dVXdd7vXxl2q6llV9dqq+kBV/UNVtap6yxke8/iquqqqjlTVrVX10ap6UVXt6Grd3KWqZqvqe6rq16rqL6rqS1X1har6YFV9d1Wt+ueY4zh5quqVVfWeqvqb4XE8UlV/UlU/WVWzazzGceyAntYPelr/6WnTQ0+bTPpZP+hn/aefTQ/9bHLpaf2gp/WfnjY9+tzTqrXW5fNNhap6SJJrk9w/ya8n+dMkj0nylCR/luQJrbWl7VshJ1XVh5N8Y5KjSW5K8rAk/29r7TvXqH96koNJjiV5W5IjSb4jyUOTvKO19uwOls0yVfV9SX4+yaeTXJPkr5PsS3JJkvtkcLye3Zb9YeY4Tqaquj3J9Un+Z5LPJrlXkguT/LMkf5fkwtba3yyrdxw7oKf1h57Wf3ra9NDTJo9+1h/6Wf/pZ9NDP5tMelp/6Gn9p6dNj173tNaa24i3JL+bpCW5fMX+/zzc//rtXqPbncfkKUm+LkkluWh4fN6yRu29M/gA35bkny3bvzuD/zhqSZ673a/pbLsluTiDPyDvtmL/AzJonC3JpY7j5N+S7F5j/88Mj8t/cRy35bjoaT256Wn9v+lp03PT0ybvpp/156af9f+mn03PTT+bzJue1p+bntb/m542Pbc+9zSXuxxRVT04yTcn+VSS/2fF8E8m+WKS76qqe3W8NFbRWrumtfbnbfgpO4NnJblfkre21v542RzHkrxkePf7x7BMTqO19t7W2m+01r68Yv9nkrx+ePeiZUOO44QaHoPVvH24/bpl+xzHDuhp/aKn9Z+eNj30tMmin/WLftZ/+tn00M8mj57WL3pa/+lp06PPPU1IN7qLh9t3rfLhvSXJh5LcM4NTKemXk8f2d1YZe3+SW5M8vqru3t2SOIPjw+0dy/Y5jv3zHcPtR5ftcxy7oadNL5+h/tHTpoOetj30s+nl89M/+tl00M+2j542vXyG+kdPmw4T39OEdKN76HD7yTXG/3y4/foO1sLWWvPYttbuSLKYZGeSB3e5KFZXVTuT/Ovh3eV/oDqOE66qfqSqrqyqV1XVB5K8IoNG+bPLyhzHbuhp08tnqEf0tP7S0yaGfja9fH56RD/rL/1souhp08tnqEf0tP7qY0/b2cWTTJn7DLdfWGP85P5zx78Utphj2y8/m+QbklzVWvvdZfsdx8n3Ixl8Ce9Jv5Pkea21zy3b5zh2w/s8vRzbftHT+ktPmwze4+nl2PaLftZf+tnk8D5PL8e2X/S0/updT3Mm3dar4XY91yKmXxzbCVFVL0zyw0n+NMl3jfrw4dZx3CattQe01iqDL+G9JIN/lfInVXX+CNM4jt3wPk8vx3ZC6Gn9pqf1hvd4ejm2E0I/6zf9rFe8z9PLsZ0Qelq/9bGnCelGdzJFvc8a4/deUUd/OLY9UFU/kOQ1Sf5nkqe01o6sKHEce6K1dri19msZfCn2bJI3LRt2HLvhfZ5ejm0P6GnTQ0/bdt7j6eXY9oB+Nj30s4ngfZ5ejm0P6GnTo089TUg3uj8bbte69vPXDbdrXTuaybXmsR1eh3gugy8KvbHLRXGXqnpRktcl+XgGjfIzq5Q5jj3TWvurDP7jZ39VfdVwt+PYDT1tevkMTTg9bTrpadtGP5tePj8TTj+bTvrZttLTppfP0ITT06ZTH3qakG501wy331xVX/H+VdVMkick+VKSP+h6YWzae4fbb11l7ElJ7pnk2tbabd0tiZOq6seSvCrJhzNolJ9do9Rx7KcHDrcnhlvHsRt62vTyGZpgetrU09O6p59NL5+fCaafTT39bHvoadPLZ2iC6WlTb6J7mpBuRK21v0zyriTnJfmBFcMvT3KvJG9qrX2x46Wxee9I8vkkz62qf3ZyZ1XtTvLTw7s/vx0LO9tV1Usz+MLW65L8i9ba509T7jhOoKp6WFU9YJX9d6uqn0ly/wya398PhxzHDuhpU81naELpaf2np00e/Wyq+fxMKP2s//SzyaSnTTWfoQmlp/Vf33tateY7DEdVVQ9Jcm0GB/fXk3wiyWOTPCWD080f31pb2r4VclJVPSPJM4Z3H5DkWzI4TfUDw32fb639yIr6dyQ5luStSY4keVqShw73P6f50HSqqv5Nkjdm8C8dXpvVrwX8qdbaG5c95hlxHCfK8JIB/zHJ+5P8ZZKlJPuSPDmDL3D9TAb/IfQ/lz3mGXEcx05P6w89rf/0tOmgp00m/aw/9LP+08+mg342ufS0/tDT+k9Pmw5972lCug2qqn+S5KcyOCVyNsmnk7wzyctX+UJJtklVXZnkJ09T8lettfNWPOYJSX4iyeOS7E7yF0l+OcnPtdZOnDIDY7WOY5gkv9dau2jF4xzHCVJV35Dk+zO4NMeDkpyb5IsZ/A/Gb2VwXE75s9Nx7Iae1g96Wv/padNBT5tc+lk/6Gf9p59NB/1ssulp/aCn9Z+eNh363tOEdAAAAAAAANAx30kHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR0AAAAAAAB0TEgHAAAAAAAAHRPSAQAAAAAAQMeEdAAAAAAAANAxIR2MUVU9r6paVT1vu9eymqr634fre8wm5rhyOMdFW7ey8aiq36iqv6yqc7Z7LQB9o6dNFj0NYOP0tMmipwFsjH42WfQzNkpIB+s0bAij3J633Ws+narak+Snk/xGa+2PVox9asVr+XJV3VxV11bVD1TVzu1Z9aa9NMlckhdu90IAtpOepqcBTAs9TU8DmAb6mX7G2ataa9u9BuiFqrpyld0vSnKfJK9JcvOKsXcmWUzyj5J8urX2hfGtbnRVdUWSn0nyhNbatSvGPpXka3LX69qRQZO5JMk9kvxaa+2SYe1XJfmqJH/dWru1q/VvVFVdleRxSR7UWvvidq8HYDvoaXoawLTQ0/Q0gGmgn+lnnL2EdLAJy5rKXGvtU9u7mvWrqh0ZNPJjrbWvX2X8U1nldVXV/iT/I4OGeVFr7fc6WfAWqqr/Nclbkzy/tfZL270egEmhp+lpANNCT9PTAKaBfqafcXZwuUsYo1rj2tDD07o/VVV7qupVVfU3VfWlqvpwVT1jWLOzqq6oqj+vqmM1uKbxvz3Nc31LVV1VVZ+vqtuG9f+xqs5dpfxAkn+S5G2jvJ7W2g1J3je8+5jh8656beiqekZVvaWqPllVX6yqo1V1XVW9sKpO+bOnqt44nOe8qnpBVX1s+LoPV9UvVNV91njdF1TVwar67PB1/1VV/Zeq+kdrvIxfT3IsyXeP8toBznZ6mp4GMC30ND0NYBroZ/oZ06Gv13eFabArydVJ9mbwB/g5Sf5VkoNV9c1J/o8kj03y20luS/LsJK+tqs+11r6iyVXVy5K8PMmRJL+Z5LNJHpnkR5I8taoe11r7h2UP+abh9oMbWHcNt2c6Dfdnk3w5yR8m+dsMTs+/OINT2R+d5LvWeNx/SPItSX4jybuSPCXJ85N87fDxdy2k6l8mOThc0zuS/FWSC5J8f5KnV9UTVv5Lo9basaq6Lsnjquo+k3Y5AICe0tNWp6cB9I+etjo9DaBf9LPV6WdMntaam5vbBm9JPpVB0zhvjfHnDceft8bjfiPJ3Zftf+Jw/5EMTu8+d9nYg5PcnuRPVsz1lOFjrl1ev+L5X7Vi/x8M98+O8rqS7E9y63DsicN9Vw7vX7Si9iGrzHu3JL8yrH/sirE3Dvf/dZKvXrZ/Z5L3D8ces2z/niSfT3Li5FqWjf3YsP5da7y+Vw3Hn7rdv0Nubm5uk3LT0/Q0Nzc3t2m56Wl6mpubm9s03PQz/czt7Li53CVsrxe11m47eae19oEMrtl83yQ/1lq7ednYjUk+lOQRNbi280kvHG6fv7x++Jg3JvlwkstWPO9XJzneWls60/qGp5W/oqrekruuC/1rw7WuqbX2l6vs+3IG/6IlGfyrldX8VGvtr5c95o4k/2149zHL6p6eZDbJ21ZZy/+dQcM/UFVfvcpzfGa4XW0MgI3R006lpwH0k552Kj0NoH/0s1PpZ0wcl7uE7XPzag0lyd8lmUty3Spjf5tkR5IHDH9OksclOZ7k2VX17FUec06S+1XV7LLmOJvk79exxh8cbluSo0k+muQtSV5/pgdW1WyS/zPJUzP41zj3WlHyj9d46B+vsu9vhtv7Ltt3/nD73pXFrbU7qur9Sc5L8qgM/pXMckeG269aYw0AjEZPW52eBtA/etrq9DSAftHPVqefMXGEdLB91rom8R1J0la/ZvEdw+2uZftmM/gs/+QZnm9PkpPN8ktJdq9jjXNtxbWV12P4pbH/I4Om/0dJ3pRBg7ojybkZNOG7r/Hwm1fZd/J1L/+XPPcZbj+9xjwn95+7ytg9htsvrfFYAEajp63u5lX26WkAk01PW93Nq+zT0wAml362uptX2aefsa2EdNB/X0hyt9ba3hEe89kkX1dVu1prx8ewpu/JoFG+vLV25fKBqnpc7vqXMptx8j8mHrDG+D9aUbfc7HD72S1YBwBbR09bnZ4G0D962ur0NIB+0c9Wp5+xZXwnHfTfHyS5b1XtH+ExHx1uHzqG9STJ1w63B1cZe/IWPcefDLcXrRyoqp1J/vnw7vWrPPZhw+2Ht2gtAGwNPW0FPQ2gt/S0FfQ0gF7Sz1bQz9hqQjrov1cNt79YVQ9cOVhV96qqC1fsft9wu3L/VvnUcHvRirU8Ksn/tUXP8c4MTmX/V6u8vhdlcD3qdy//MthlLkzy+SQf36K1ALA19DQ9DWBa6Gl6GsA00M/0M8ZMSAc911p7T5Ifz6AB/HlV/WpV/Yeq+i9V9VtJDie5csXD3pnkRJJvGdOyTl4L+tVVdaiqXllVh5L8YZLf3oonaK0dTfK/J/lykt+rqrdU1b+rqt9N8h+TfCbJC1Y+rqoemuSrkxxqrbWtWAsAW0NP09MApoWepqcBTAP9TD9j/HwnHUyB1torq+pDSV6YwenWT8/gmsh/m+QXkiysqL+pqn4jyXdU1X1ba3+/xev5u6p6YpKfHa7nW5L8aZL/I8m7k/yvW/Q8v15VT0hyxfA57pNBk3x9kle01v5ulYf9m+H257diDQBsLT1NTwOYFnqangYwDfQz/YzxKoEunJ2q6vFJPpTkxa21V52pfhpU1d2T3JjkE621b9ru9QCwNfQ0PQ1gWuhpehrANNDP9DPWz+Uu4SzVWrs2ya8m+bGquud2r6cj35/kAUl+eLsXAsDW0dMAmBZ6GgDTQD+D9RPSwdntRzI4RXtuuxfSkduSfHdr7SPbvRAAtpyeBsC00NMAmAb6GayDy10CAAAAAABAx5xJBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDHhHQAAAAAAADQMSEdAAAAAAAAdExIBwAAAAAAAB0T0gEAAAAAAEDH/n9pAEWCZ50tnwAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 884
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create a Tensorflow dataset object from our numpy array to feed into our model. The dataset object helps us feed batches of data into our model. A batch is a subset of the data that is passed through the deep learning network before the weights are updated. Batching data is necessary in most training scenarios as our training environment might not be able to load the entire dataset into memory at once."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Number of input data samples in a batch\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Shuffle buffer size for shuffling data\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "#Preloads PREFETCH_SIZE batches so that there is no idle time between batches\n",
    "PREFETCH_SIZE = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_dataset(filename):\n",
    "    \n",
    "    \"\"\"Load the samples used for training.\"\"\"\n",
    "    \n",
    "    data = np.load(filename)\n",
    "    data = np.asarray(data, dtype=np.float32)  # {-1, 1}\n",
    "\n",
    "    print('data shape = {}'.format(data.shape))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(PREFETCH_SIZE)\n",
    "\n",
    "    return dataset \n",
    "\n",
    "dataset = prepare_dataset('./dataset/train.npy')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data shape = (229, 32, 128, 4)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 19:14:26.024225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-02 19:14:26.024264: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-02 19:14:26.024283: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tuf-ml-station-fx505dy): /proc/driver/nvidia/version does not exist\n",
      "2021-08-02 19:14:26.025363: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model architecture\n",
    "In this section, we will walk through the architecture of the proposed GAN.\n",
    "\n",
    "The model consists of two networks, a generator and a critic. These two networks work in a tight loop as following:\n",
    "\n",
    "* Generator:\n",
    "    1. The generator takes in a batch of single-track piano rolls (melody) as the input and generates a batch of multi-track piano rolls as the output by adding accompaniments to each of the input music tracks. \n",
    "    2. The critic then takes these generated music tracks and predicts how far it deviates from the real data present in your training dataset.\n",
    "    3. This feedback from the critic is used by the generator to update its weights.\n",
    "* Critic: As the generator gets better at creating better music accompaniments using the feedback from the critic, the critic needs to be retrained as well.\n",
    "    1. Train the critic with the music tracks just generated by the generator as fake inputs and an equivalent number of songs from the original dataset as the real input. \n",
    "* Alternate between training these two networks until the model converges and produces realistic music, beginning with the critic on the first iteration.\n",
    "\n",
    "We use a special type of GAN called the **Wasserstein GAN with Gradient Penalty** (or **WGAN-GP**) to generate music. While the underlying architecture of a WGAN-GP is very similar to vanilla variants of GAN, WGAN-GPs help overcome some of the commonly seen defects in GANs such as the vanishing gradient problem and mode collapse (see appendix for more details).\n",
    "\n",
    "Note our \"critic\" network is more generally called a \"discriminator\" network in the more general context of vanilla GANs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The generator is adapted from the U-Net architecture (a popular CNN that is used extensively in the computer vision domain), consisting of an “encoder” that maps the single track music data (represented as piano roll images) to a relatively lower dimensional “latent space“ and a ”decoder“ that maps the latent space back to multi-track music data.\n",
    "\n",
    "Here are the inputs provided to the generator:\n",
    "\n",
    "**Single-track piano roll input**: A single melody track of size (32, 128, 1) => (TimeStep, NumPitches, NumTracks) is provided as the input to the generator. \n",
    "\n",
    "**Latent noise vector**: A latent noise vector z of dimension (2, 8, 512) is also passed in as input and this is responsible for ensuring that there is a distinctive flavor to each output generated by the generator, even when the same input is provided.\n",
    "\n",
    "Notice from the figure below that the encoding layers of the generator on the left side and decoder layer on on the right side are connected to create a U-shape, thereby giving the name U-Net to this architecture."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/dgen.png\" alt=\"Generator architecture\" width=\"800\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this implementation, we build the generator following a simple four-level Unet architecture by combining `_conv2d`s and `_deconv2d`, where `_conv2d` compose the contracting path and `_deconv2d` forms the expansive path. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def _conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "    \"\"\"Generator Basic Downsampling Block\"\"\"\n",
    "    d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2,\n",
    "                               padding='same')(layer_input)\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "    if bn:\n",
    "        d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _deconv2d(layer_input, pre_input, filters, f_size=4, dropout_rate=0):\n",
    "    \"\"\"Generator Basic Upsampling Block\"\"\"\n",
    "    u = tf.keras.layers.UpSampling2D(size=2)(layer_input)\n",
    "    u = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=1,\n",
    "                               padding='same')(u)\n",
    "    u = tf.keras.layers.BatchNormalization(momentum=0.8)(u)\n",
    "    u = tf.keras.layers.ReLU()(u)\n",
    "\n",
    "    if dropout_rate:\n",
    "        u = tf.keras.layers.Dropout(dropout_rate)(u)\n",
    "        \n",
    "    u = tf.keras.layers.Concatenate()([u, pre_input])\n",
    "    return u\n",
    "\n",
    "    \n",
    "def build_generator(condition_input_shape=(32, 128, 1), filters=64,\n",
    "                    instruments=4, latent_shape=(2, 8, 512)):\n",
    "    \"\"\"Buld Generator\"\"\"\n",
    "    c_input = tf.keras.layers.Input(shape=condition_input_shape)\n",
    "    z_input = tf.keras.layers.Input(shape=latent_shape)\n",
    "\n",
    "    d1 = _conv2d(c_input, filters, bn=False)\n",
    "    d2 = _conv2d(d1, filters * 2)\n",
    "    d3 = _conv2d(d2, filters * 4)\n",
    "    d4 = _conv2d(d3, filters * 8)\n",
    "\n",
    "    d4 = tf.keras.layers.Concatenate(axis=-1)([d4, z_input])\n",
    "\n",
    "    u4 = _deconv2d(d4, d3, filters * 4)\n",
    "    u5 = _deconv2d(u4, d2, filters * 2)\n",
    "    u6 = _deconv2d(u5, d1, filters)\n",
    "\n",
    "    u7 = tf.keras.layers.UpSampling2D(size=2)(u6)\n",
    "    output = tf.keras.layers.Conv2D(instruments, kernel_size=4, strides=1,\n",
    "                               padding='same', activation='tanh')(u7)  # 32, 128, 4\n",
    "\n",
    "    generator = tf.keras.models.Model([c_input, z_input], output, name='Generator')\n",
    "\n",
    "    return generator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now dive into each layer of the generator to see the inputs/outputs at each layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Models\n",
    "generator = build_generator()\n",
    "generator.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 128, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 64, 64)   1088        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 16, 64, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 32, 128)   131200      leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 8, 32, 128)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8, 32, 128)   512         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 16, 256)   524544      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 4, 16, 256)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 16, 256)   1024        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2, 8, 512)    2097664     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 2, 8, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2, 8, 512)    2048        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2, 8, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 8, 1024)   0           batch_normalization_2[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 4, 16, 1024)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 16, 256)   4194560     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 4, 16, 256)   1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 4, 16, 256)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 16, 512)   0           re_lu[0][0]                      \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 8, 32, 512)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 32, 128)   1048704     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 8, 32, 128)   512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 8, 32, 128)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 32, 256)   0           re_lu_1[0][0]                    \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 16, 64, 256)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 64, 64)   262208      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 16, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 64, 128)  0           re_lu_2[0][0]                    \n",
      "                                                                 leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 32, 128, 128) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 128, 4)   8196        up_sampling2d_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 8,273,540\n",
      "Trainable params: 8,270,852\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Critic (Discriminator)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of the critic is to provide feedback to the generator about how realistic the generated piano rolls are, so that the generator can learn to produce more realistic data. The critic provides this feedback by outputting a scalar that  represents how “real” or “fake” a piano roll is.\n",
    "\n",
    "Since the critic tries to classify data as “real” or “fake”, it is not very different from commonly used binary classifiers.  We use a simple architecture for the critic, composed of four convolutional layers and a dense layer at the end."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/ddis.png\" alt=\"Discriminator architecture\" width=\"800\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def _build_critic_layer(layer_input, filters, f_size=4):\n",
    "    \"\"\"\n",
    "    This layer decreases the spatial resolution by 2:\n",
    "\n",
    "        input:  [batch_size, in_channels, H, W]\n",
    "        output: [batch_size, out_channels, H/2, W/2]\n",
    "    \"\"\"\n",
    "    d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2,\n",
    "                               padding='same')(layer_input)\n",
    "    # Critic does not use batch-norm\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d) \n",
    "    return d\n",
    "\n",
    "\n",
    "def build_critic(pianoroll_shape=(32, 128, 4), filters=64):\n",
    "    \"\"\"WGAN critic.\"\"\"\n",
    "    \n",
    "    condition_input_shape = (32,128,1)\n",
    "    groundtruth_pianoroll = tf.keras.layers.Input(shape=pianoroll_shape)\n",
    "    condition_input = tf.keras.layers.Input(shape=condition_input_shape)\n",
    "    combined_imgs = tf.keras.layers.Concatenate(axis=-1)([groundtruth_pianoroll, condition_input])\n",
    "\n",
    "\n",
    "    \n",
    "    d1 = _build_critic_layer(combined_imgs, filters)\n",
    "    d2 = _build_critic_layer(d1, filters * 2)\n",
    "    d3 = _build_critic_layer(d2, filters * 4)\n",
    "    d4 = _build_critic_layer(d3, filters * 8)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(d4)\n",
    "    logit = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    critic = tf.keras.models.Model([groundtruth_pianoroll,condition_input], logit,\n",
    "                                          name='Critic')\n",
    "    \n",
    "\n",
    "    return critic"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the Discriminator\n",
    "\n",
    "critic = build_critic()\n",
    "critic.summary() # View discriminator architecture."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Critic\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 128, 4)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 32, 128, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 128, 5)   0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 64, 64)   5184        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 16, 64, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 32, 128)   131200      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 8, 32, 128)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 16, 256)   524544      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 4, 16, 256)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 8, 512)    2097664     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 2, 8, 512)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            8193        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,766,785\n",
      "Trainable params: 2,766,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "We train our models by searching for model parameters which optimize an objective function. For our WGAN-GP, we have special loss functions that we minimize as we alternate between training our generator and critic networks:\n",
    "\n",
    "*Generator Loss:*\n",
    "* We use the Wasserstein (Generator) loss function which is negative of the Critic Loss function. The generator is trained to bring the generated pianoroll as close to the real pianoroll as possible.\n",
    "    * $\\frac{1}{m} \\sum_{i=1}^{m} -D_w(G(z^{i}|c^{i})|c^{i})$\n",
    "\n",
    "*Critic Loss:*\n",
    "\n",
    "* We begin with the Wasserstein (Critic) loss function designed to maximize the distance between the real piano roll distribution and generated (fake) piano roll distribution.\n",
    "    * $\\frac{1}{m} \\sum_{i=1}^{m} [D_w(G(z^{i}|c^{i})|c^{i}) - D_w(x^{i}|c^{i})]$\n",
    "\n",
    "* We add a gradient penalty loss function term designed to control how the gradient of the critic with respect to its input behaves.  This makes optimization of the generator easier. \n",
    "    * $\\frac{1}{m} \\sum_{i=1}^{m}(\\lVert \\nabla_{\\hat{x}^i}D_w(\\hat{x}^i|c^{i}) \\rVert_2 -  1)^2 $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the different loss functions\n",
    "\n",
    "def generator_loss(critic_fake_output):\n",
    "    \"\"\" Wasserstein GAN loss\n",
    "    (Generator)  -D(G(z|c))\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(critic_fake_output)\n",
    "\n",
    "\n",
    "def wasserstein_loss(critic_real_output, critic_fake_output):\n",
    "    \"\"\" Wasserstein GAN loss\n",
    "    (Critic)  D(G(z|c)) - D(x|c)\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(critic_fake_output) - tf.reduce_mean(\n",
    "        critic_real_output)\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(critic, x, fake_x):\n",
    "    \n",
    "    c = tf.expand_dims(x[..., 0], -1)\n",
    "    batch_size = x.get_shape().as_list()[0]\n",
    "    eps_x = tf.random.uniform(\n",
    "        [batch_size] + [1] * (len(x.get_shape()) - 1))  # B, 1, 1, 1, 1\n",
    "    inter = eps_x * x + (1.0 - eps_x) * fake_x\n",
    "\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(inter)\n",
    "        disc_inter_output = critic((inter,c), training=True)\n",
    "    grads = g.gradient(disc_inter_output, inter)\n",
    "    slopes = tf.sqrt(1e-8 + tf.reduce_sum(\n",
    "        tf.square(grads),\n",
    "        reduction_indices=tf.range(1, grads.get_shape().ndims)))\n",
    "    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n",
    "    \n",
    "    return gradient_penalty\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our loss functions defined, we associate them with Tensorflow optimizers to define how our model will search for a good set of model parameters. We use the *Adam* algorithm, a commonly used general-purpose optimizer. We also set up checkpoints to save our progress as we train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "# We define our checkpoint directory and where to save trained checkpoints\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                           generator_optimizer=generator_optimizer,\n",
    "                           critic=critic,\n",
    "                           critic_optimizer=critic_optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, check_dir, max_to_keep=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define the `generator_train_step` and `critic_train_step` functions, each of which performs a single forward pass on a batch and returns the corresponding loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\n",
    "def generator_train_step(x, condition_track_idx=0):\n",
    "\n",
    "    ############################################\n",
    "    #(1) Update G network: maximize D(G(z|c))\n",
    "    ############################################\n",
    "\n",
    "    # Extract condition track to make real batches pianoroll\n",
    "    c = tf.expand_dims(x[..., condition_track_idx], -1)\n",
    "\n",
    "    # Generate batch of latent vectors\n",
    "    z = tf.random.truncated_normal([BATCH_SIZE, 2, 8, 512])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_x = generator((c, z), training=True)\n",
    "        fake_output = critic((fake_x,c), training=False)\n",
    "\n",
    "        # Calculate Generator's loss based on this generated output\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    # Calculate gradients for Generator\n",
    "    gradients_of_generator = tape.gradient(gen_loss,\n",
    "                                           generator.trainable_variables)\n",
    "    # Update Generator\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\n",
    "def critic_train_step(x, condition_track_idx=0):\n",
    "\n",
    "    ############################################################################\n",
    "    #(2) Update D network: maximize (D(x|c)) + (1 - D(G(z|c))|c) + GradientPenality() \n",
    "    ############################################################################\n",
    "\n",
    "    # Extract condition track to make real batches pianoroll\n",
    "    c = tf.expand_dims(x[..., condition_track_idx], -1)\n",
    "\n",
    "    # Generate batch of latent vectors\n",
    "    z = tf.random.truncated_normal([BATCH_SIZE, 2, 8, 512])\n",
    "\n",
    "    # Generated fake pianoroll\n",
    "    fake_x = generator((c, z), training=False)\n",
    "\n",
    "\n",
    "    # Update critic parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_output = critic((x,c), training=True)\n",
    "        fake_output = critic((fake_x,c), training=True)\n",
    "        critic_loss =  wasserstein_loss(real_output, fake_output)\n",
    "\n",
    "    # Caculate the gradients from the real and fake batches\n",
    "    grads_of_critic = tape.gradient(critic_loss,\n",
    "                                               critic.trainable_variables)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        gp_loss = compute_gradient_penalty(critic, x, fake_x)\n",
    "        gp_loss *= 10.0\n",
    "\n",
    "    # Calculate the gradients penalty from the real and fake batches\n",
    "    grads_gp = tape.gradient(gp_loss, critic.trainable_variables)\n",
    "    gradients_of_critic = [g + ggp for g, ggp in\n",
    "                                  zip(grads_of_critic, grads_gp)\n",
    "                                  if ggp is not None]\n",
    "\n",
    "    # Update Critic\n",
    "    critic_optimizer.apply_gradients(\n",
    "        zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "    return critic_loss + gp_loss\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we begin training, let's define some training configuration parameters and prepare to monitor important quantities. Here we log the losses and metrics which we can use to determine when to stop training. Consider coming back here to tweak these parameters and explore how your model responds. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We use load_melody_samples() to load 10 input data samples from our dataset into sample_x \n",
    "# and 10 random noise latent vectors into sample_z\n",
    "sample_x, sample_z = inference_utils.load_melody_samples(n_sample=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 10 melody samples\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Number of iterations to train for\n",
    "iterations = 1000\n",
    "\n",
    "# Update critic n times per generator update \n",
    "n_dis_updates_per_gen_update = 5\n",
    "\n",
    "# Determine input track in sample_x that we condition on\n",
    "condition_track_idx = 0 \n",
    "sample_c = tf.expand_dims(sample_x[..., condition_track_idx], -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now train our model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Clear out any old metrics we've collected\n",
    "metrics_utils.metrics_manager.initialize()\n",
    "\n",
    "# Keep a running list of various quantities:\n",
    "c_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Data iterator to iterate over our dataset\n",
    "it = iter(dataset)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    # Train critic\n",
    "    for _ in range(n_dis_updates_per_gen_update):\n",
    "        c_loss = critic_train_step(next(it))\n",
    "\n",
    "    # Train generator\n",
    "    g_loss = generator_train_step(next(it))\n",
    "\n",
    "    # Save Losses for plotting later\n",
    "    c_losses.append(c_loss)\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    line1, = plt.plot(range(iteration+1), c_losses, 'r')\n",
    "    line2, = plt.plot(range(iteration+1), g_losses, 'k')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.legend((line1, line2), ('C-loss', 'G-loss'))\n",
    "    display.display(fig)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Output training stats\n",
    "    print('Iteration {}, c_loss={:.2f}, g_loss={:.2f}'.format(iteration, c_loss, g_loss))\n",
    "    \n",
    "    # Save checkpoints, music metrics, generated output\n",
    "    if iteration < 100 or iteration % 50 == 0 :\n",
    "        # Check how the generator is doing by saving G's samples on fixed_noise\n",
    "        fake_sample_x = generator((sample_c, sample_z), training=False)\n",
    "        metrics_utils.metrics_manager.append_metrics_for_iteration(fake_sample_x.numpy(), iteration)\n",
    "\n",
    "        if iteration % 50 == 0:\n",
    "            # Save the checkpoint to disk.\n",
    "            ckpt_manager.save(checkpoint_number=iteration) \n",
    "        \n",
    "            fake_sample_x = fake_sample_x.numpy()\n",
    "    \n",
    "            # plot the pianoroll\n",
    "            display_utils.plot_pianoroll(iteration, sample_x[:4], fake_sample_x[:4], save_dir=train_dir)\n",
    "\n",
    "            # generate the midi\n",
    "            destination_path = path_utils.generated_midi_path_for_iteration(iteration, saveto_dir=sample_dir)\n",
    "            midi_utils.save_pianoroll_as_midi(fake_sample_x[:4], destination_path=destination_path)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_23201/2934098770.py:29 critic_train_step  *\n        gp_loss = compute_gradient_penalty(critic, x, fake_x)\n    /tmp/ipykernel_23201/2942989480.py:30 compute_gradient_penalty  *\n        slopes = tf.sqrt(1e-8 + tf.reduce_sum(\n    /home/jesus/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n\n    TypeError: reduce_sum() got an unexpected keyword argument 'reduction_indices'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23201/3064720895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Train critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_dis_updates_per_gen_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_23201/2934098770.py:29 critic_train_step  *\n        gp_loss = compute_gradient_penalty(critic, x, fake_x)\n    /tmp/ipykernel_23201/2942989480.py:30 compute_gradient_penalty  *\n        slopes = tf.sqrt(1e-8 + tf.reduce_sum(\n    /home/jesus/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n\n    TypeError: reduce_sum() got an unexpected keyword argument 'reduction_indices'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We have started training!\n",
    "\n",
    "When using the Wasserstein loss function, we should train the critic to converge to ensure that the gradients for the generator update are accurate. This is in contrast to a standard GAN, where it is important not to let the critic get too strong, to avoid vanishing gradients.\n",
    "\n",
    "Therefore, using the Wasserstein loss removes one of the key difficulties of training GANs—how to balance the training of the discriminator and generator. With WGANs, we can simply train the critic several times between generator updates, to ensure it is close to convergence. A typical ratio used is five critic updates to one generator update.\n",
    "\n",
    "### \"Babysitting\" the learning process\n",
    "\n",
    "Given that training these models can be an investment in time and resources, we must to continuously monitor training in order to catch and address anomalies if/when they occur. Here are some things to look out for:\n",
    "\n",
    "**What should the losses look like?**\n",
    "\n",
    "The adversarial learning process is highly dynamic and high-frequency oscillations are quite common. However if either loss (critic or generator) skyrockets to huge values, plunges to 0, or get stuck on a single value, there is likely an issue somewhere.\n",
    "\n",
    "**Is my model learning?**\n",
    "- Monitor the critic loss and other music quality metrics (if applicable). Are they following the expected trajectories?\n",
    "- Monitor the generated samples (piano rolls). Are they improving over time? Do you see evidence of mode collapse? Have you tried listening to your samples?\n",
    "\n",
    "**How do I know when to stop?**\n",
    "- If the samples meet your expectations\n",
    "- Critic loss no longer improving\n",
    "- The expected value of the musical quality metrics converge to the corresponding expected value of the same metric on the training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How to measure sample quality during training \n",
    "\n",
    "Typically, when training any sort of neural networks, it is standard practice to monitor the value of the loss function throughout the duration of the training. The critic loss in WGANs has been found to correlate well with sample quality.\n",
    "\n",
    "While standard mechanisms exist for evaluating the accuracy of more traditional models like classifiers or regressors, evaluating generative models is an active area of research. Within the domain of music generation, this hard problem is even less well-understood.\n",
    "\n",
    "To address this, we take high-level measurements of our data and show how well our model produces music that aligns with those measurements. If our model produces music which is close to the mean value of these measurements for our training dataset, our music should match on general “shape”.\n",
    "\n",
    "We’ll look at three such measurements:\n",
    "- **Empty bar rate:** The ratio of empty bars to total number of bars.\n",
    "- **Pitch histogram distance:** A metric that captures the distribution and position of pitches.\n",
    "- **In Scale Ratio:** Ratio of the number of notes that are in C major key, which is a common key found in music, to the total number of notes. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate results\n",
    "\n",
    "Now that we have finished training, let's find out how we did. We will analyze our model in several ways:\n",
    "1. Examine how the generator and critic losses changed while training\n",
    "2. Understand how certain musical metrics changed while training\n",
    "3. Visualize generated piano roll output for a fixed input at every iteration and create a video\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us first restore our last saved checkpoint. If you did not complete training but still want to continue with a pre-trained version, set `TRAIN = False`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ckpt = tf.train.Checkpoint(generator=generator)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, check_dir, max_to_keep=5)\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "print('Latest checkpoint {} restored.'.format(ckpt_manager.latest_checkpoint))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot losses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_utils.plot_loss_logs(g_losses, c_losses, figsize=(15, 5), smoothing=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe how the critic loss (C_loss in the graph) decays to zero as we train. In WGAN-GPs, the critic loss decreases (almost) monotonically as you train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metrics_utils.metrics_manager.set_reference_metrics(training_data)\n",
    "metrics_utils.metrics_manager.plot_metrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each row here corresponds to a different music quality metric and each column denotes an instrument track. \n",
    "\n",
    "Observe how the expected value of the different metrics (blue scatter) approach the corresponding training set expected values (red) as the number of iterations increase. You might expect to see diminishing returns as the model converges.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generated samples during training\n",
    "\n",
    "The function below helps you probe intermediate samples generated in the training process. Remember that the conditioned input here is sampled from our training data. Let's start by listening to and observing a sample at iteration 0 and then iteration 100. Notice the difference!\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Enter an iteration number (can be divided by 50) and listen to the midi at that iteration\n",
    "iteration = 50\n",
    "midi_file = os.path.join(sample_dir, 'iteration-{}.mid'.format(iteration))\n",
    "display_utils.playmidi(midi_file)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Enter an iteration number (can be divided by 50) and look at the generated pianorolls at that iteration\n",
    "iteration = 50\n",
    "pianoroll_png = os.path.join(train_dir, 'sample_iteration_%05d.png' % iteration)\n",
    "display.Image(filename=pianoroll_png)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how the generated piano rolls change with the number of iterations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "\n",
    "display_utils.make_training_video(train_dir)\n",
    "video_path = \"movie.mp4\"\n",
    "Video(video_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating accompaniment for custom input\n",
    "\n",
    "Congratulations! You have trained your very own WGAN-GP to generate music. Let us see how our generator performs on a custom input.\n",
    "\n",
    "The function below generates a new song based on \"Twinkle Twinkle Little Star\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "latest_midi = inference_utils.generate_midi(generator, eval_dir, input_midi_file='./input_twinkle_twinkle.mid')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_utils.playmidi(latest_midi)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also take a look at the generated piano rolls for a certain sample, to see how diverse they are!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inference_utils.show_generated_pianorolls(generator, eval_dir, input_midi_file='./input_twinkle_twinkle.mid')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What's next?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using your own data  (Optional)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create your own dataset you can extract the piano roll from MIDI data. An example of creating a piano roll from a MIDI file is given below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from pypianoroll import Multitrack\n",
    "\n",
    "midi_data = Multitrack('./input_twinkle_twinkle.mid')\n",
    "tracks = [track.pianoroll for track in midi_data.tracks]\n",
    "sample = np.stack(tracks, axis=-1)\n",
    "\n",
    "print(sample.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open source implementations\n",
    "For more open-source implementations of generative models for music, check out:\n",
    "\n",
    "- [MuseGAN](https://github.com/salu133445/musegan): Official TensorFlow Implementation that uses GANs to generate multi track polyphonic music\n",
    "- [GANSynth](https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth): GANSynth uses a Progressive GAN architecture to incrementally upsample with convolution from a single vector to the full audio spectrogram\n",
    "- [Music Transformer](https://github.com/tensorflow/magenta/tree/master/magenta/models/score2perf): Uses transformers to generate music!\n",
    "\n",
    "GANs have also achieved state of the generative modeling in several other domains including cross domain image tranfer, celebrity face generation, super resolution text to image and image inpainting.\n",
    "\n",
    "- [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN): Library of reference implementations in Keras for image generation(good for educational purposes).\n",
    "\n",
    "There's an ocean of literatures out there that use GANs for modeling distributions across fields! If you are interested, [Gan Zoo](https://github.com/hindupuravinash/the-gan-zoo) is a good place to start."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References\n",
    "<a id='references'></a>\n",
    "1. [Dong, H.W., Hsiao, W.Y., Yang, L.C. and Yang, Y.H., 2018, April. MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In Thirty-Second AAAI Conference on Artificial Intelligence.](https://arxiv.org/abs/1709.06298)\n",
    "2. [Ishaan, G., Faruk, A., Martin, A., Vincent, D. and Aaron, C., 2017. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems.](https://arxiv.org/abs/1704.00028)\n",
    "3. [Arjovsky, M., Chintala, S. and Bottou, L., 2017. Wasserstein gan. arXiv preprint arXiv:1701.07875.](https://arxiv.org/abs/1701.07875)\n",
    "4. [Foster, D., 2019. Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play. O'Reilly Media.](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More on Wassertein GAN with Gradient Penalty (optional)\n",
    "\n",
    "While GANs are a major breakthrough for generative modeling, plain GANs are also notoriously difficult to train. Some common problems encountered are:\n",
    "\n",
    "* **Oscillating loss:** The loss of the discriminator and generator can start to oscillate without exhibiting any long term stability.\n",
    "* **Mode collapse:**  The generator may get stuck on a small set of samples that always fool the discriminator. This reduces the capability of the network to produce novel samples.\n",
    "* **Uninformative loss:** The lack of correlation between the generator loss and quality of generated output makes plain GAN training difficult to interpret.\n",
    "\n",
    "\n",
    "The [Wasserstein GAN](#references) was a major advancement in GANs and helped mitigate to some of these issues. Some of its features are:\n",
    "\n",
    "1. It significantly improves the interpretability of loss functions and provides clearer stopping criteria\n",
    "2. WGANs generally produce results of higher quality (demonstrated within the image generation domain)\n",
    "\n",
    "**Mathematics of Wasserstein GAN with Gradient Penalty**\n",
    "\n",
    "The [Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric) between the true distribution $P_r$ and generated piano roll distribution $P_g$ is defined as follows:\n",
    "\n",
    "$$\\mathbb{W}(P_{r},P_{g})=\\sup_{\\lVert{f} \\rVert_{L} \\le 1} \\mathbb{E}_{x \\sim \\mathbb{P}_r}(f(x)) - \\mathbb{E}_{x \\sim \\mathbb{P}_g}(f(x)) $$\n",
    "\n",
    "In this equation we are trying to minimize the distance between the expectation of the real distribution and the expectation of the generation distribution. $f$ is subject to a technical constraint in that it must be [1-Lipschitz](https://en.wikipedia.org/wiki/Lipschitz_continuity).\n",
    "\n",
    "To enforce the 1-Lipschitz condition that basically constraints the gradients from varying too rapidly we use the gradient penalty.\n",
    "\n",
    "**Gradient penalty**: We want to penalize the gradients of the critic. We implicitly define $P_{\\hat{x}}$ by sampling uniformly along straight lines between pairs of points sampled from the data distribution $P_r$ and the generator distribution $P_g$.   This was originally motivated by the fact that the optimal critic contains straight lines with gradient norm 1 connecting coupled points from $P_r$ and $P_g$. We use a penalty coefficient $\\lambda$= 10 as was recommended in the original paper. \n",
    "\n",
    "The loss with gradient penalty is:\n",
    "\n",
    "$$\\mathbb{L}(P_{r},P_{g},P_{\\hat{x}} )= \\mathbb{W}(P_{r},P_{g}) + \\lambda \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_\\hat{x}}[(\\lVert \\nabla_{\\hat{x}}D(\\hat{x}) \\rVert_2 -  1)^2]$$\n",
    "|\n",
    "This loss can be parametrized in terms of $w$ and $\\theta$. We then use neural networks to learn the functions $f_w$ (discriminator) and  $g_\\theta$ (generator).\n",
    "$$\\mathbb{W}(P_{r},P_{\\theta})=\\max_{w \\in \\mathbb{W}} \\mathbb{E}_{x \\sim \\mathbb{P}_r}(D_w(x)) - \\mathbb{E}_{z \\sim p(z)}(D_w(G_{\\theta}(z)) $$\n",
    "$$\\mathbb{L}(P_{r},P_{\\theta},P_{\\hat{x}})=\\max_{w \\in \\mathbb{W}} \\mathbb{E}_{x \\sim \\mathbb{P}_r}(D_w(x)) - \\mathbb{E}_{z \\sim p(z)}(D_w(G_{\\theta}(z)) + \\lambda \\mathbb{E}_{\\hat{x} \\sim \\mathbb{P}_\\hat{x}}[(\\lVert \\nabla_{\\hat{x}}D_w(\\hat{x}) \\rVert_2 -  1)^2]$$\n",
    "\n",
    "where $$ \\hat{x} = \\epsilon x + (1- \\epsilon) G(z) $$ and $$\\epsilon \\sim Unif(0,1)$$\n",
    "\n",
    "The basic procedure to train is as following: \n",
    "1. We draw real_x from the real distribution $P_r$ and fake_x from the generated distribution $G_{\\theta}(z)$ where $z \\sim p(z)$\n",
    "2. The latent vectors are sampled from z and then tranformed using the generator $G_{\\theta}$ to get the fake samples fake_x. They are evaluated using the critic function $D_w$\n",
    "3. We are trying to minimize the Wasserstein distance between the two distributions\n",
    "\n",
    "Both the generator and critic are conditioned on the input pianoroll melody."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c304d0744c63eb0cf275252f889b831c4efa7f5cd03bb8eb1791a6cbed8475e"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}